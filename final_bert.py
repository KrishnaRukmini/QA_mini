# -*- coding: utf-8 -*-
"""Final_Bert

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OVj0wVbn8AB0YJYS-bj7UW2VR-n-viuq
"""

! pip install -U tokenizers

!pip install tensorflow==1.15

import tokenizers

!pip install sentencepiece
!git clone https://github.com/google-research/bert

import os
import sys
import json
import nltk
import random
import tokenizers
import logging
import tensorflow as tf
import sentencepiece as spm

from glob import glob
from google.colab import auth, drive
from tensorflow.keras.utils import Progbar

sys.path.append("bert")

from bert import modeling, optimization, tokenization
from bert.run_pretraining import input_fn_builder, model_fn_builder

auth.authenticate_user()
  
# configure logging
log = logging.getLogger('tensorflow')
log.setLevel(logging.INFO)

# create formatter and add it to the handlers
formatter = logging.Formatter('%(asctime)s :  %(message)s')
sh = logging.StreamHandler()
sh.setLevel(logging.INFO)
sh.setFormatter(formatter)
log.handlers = [sh]

if 'COLAB_TPU_ADDR' in os.environ:
  log.info("Using TPU runtime")
  USE_TPU = True
  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']

  with tf.Session(TPU_ADDRESS) as session:
    log.info('TPU address is ' + TPU_ADDRESS)
    # Upload credentials to TPU.
    with open('/content/adc.json', 'r') as f:
      auth_info = json.load(f)
    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)
    
else:
  log.warning('Not connected to TPU runtime')
  USE_TPU = False

import nltk
regex_tokenizer = nltk.RegexpTokenizer("\w+")

def normalize_text(text):
  # lowercase text
  text = str(text).lower()
  # remove punktuation symbols
  text = " ".join(regex_tokenizer.tokenize(text))
  return text

def count_lines(filename):
  count = 0
  with open(filename) as fi:
    for line in fi:
      count += 1
  return count

RAW_DATA_FPATH = "myfile.txt"
PRC_DATA_FPATH = "proc_dataset.txt" 

# apply normalization to the dataset
# this will take a minute or two

total_lines = count_lines(RAW_DATA_FPATH)
bar = Progbar(total_lines)

with open(RAW_DATA_FPATH) as fi:
  with open(PRC_DATA_FPATH, "w") as fo:
    for l in fi:
      fo.write(normalize_text(l)+"\n")
      bar.add(1)

import sentencepiece as spm
PRC_DATA_FPATH = "proc_dataset.txt"

MODEL_PREFIX = "tokenizer" 
VOC_SIZE = 32000 
SUBSAMPLE_SIZE = 1280000 
NUM_PLACEHOLDERS = 256 

SPM_COMMAND = ('--input={} --model_prefix={} '
               '--vocab_size={} --input_sentence_size={} '
               '--shuffle_input_sentence=true ' 
               '--bos_id=-1 --eos_id=-1').format(
               PRC_DATA_FPATH, MODEL_PREFIX, 
               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)

spm.SentencePieceTrainer.Train(SPM_COMMAND)

def read_sentencepiece_vocab(filepath):
  voc = []
  with open(filepath) as fi:
    for line in fi:
      voc.append(line.split("\t")[0])
  voc = voc[1:]
  return voc

snt_vocab = read_sentencepiece_vocab("{}.vocab".format(MODEL_PREFIX))

def parse_sentencepiece_token(token):
    if token.startswith("▁"):
        return token[1:]
    else:
        return "##" + token

bert_vocab = list(map(parse_sentencepiece_token, snt_vocab))
token_sub = ["[PAD]","[UNK]","[CLS]","[SEP]","[MASK]"]
bert_vocab = token_sub + bert_vocab
bert_vocab += ["[UNUSED_{}]".format(i) for i in range(VOC_SIZE - len(bert_vocab))]





VOC_FNAME = "vocab.txt" 

with open(VOC_FNAME, "w") as fo:
  for token in bert_vocab:
    fo.write(token+"\n")

bert_tokenizer = tokenization.FullTokenizer(VOC_FNAME)

!mkdir ./shards
!split -a 4 -l 256000 -d $PRC_DATA_FPATH ./shards/shard_

MAX_SEQ_LENGTH = 128 
MASKED_LM_PROB = 0.15 
MAX_PREDICTIONS = 20 
DO_LOWER_CASE = True 
PROCESSES = 2 
PRETRAINING_DIR = "pretraining_data"

XARGS_CMD = ("ls ./shards/ | "
             "xargs -n 1 -P {} -I{} "
             "python3 bert/create_pretraining_data.py "
             "--input_file=./shards/{} "
             "--output_file={}/{}.tfrecord "
             "--vocab_file={} "
             "--do_lower_case={} "
             "--max_predictions_per_seq={} "
             "--max_seq_length={} "
             "--masked_lm_prob={} "
             "--random_seed=1234 "
             "--dupe_factor=5")

XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', 
                             VOC_FNAME, DO_LOWER_CASE, 
                             MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)

tf.gfile.MkDir(PRETRAINING_DIR)
!$XARGS_CMD

BUCKET_NAME = "another_qa_bucket" 
MODEL_DIR = "bert_model_wiki" 
tf.gfile.MkDir(MODEL_DIR)

if not BUCKET_NAME:
  log.warning("WARNING: BUCKET_NAME is not set. "
              "You will not be able to train the model.")

# use this for BERT-base

bert_base_config = {
  "attention_probs_dropout_prob": 0.1, 
  "directionality": "bidi", 
  "hidden_act": "gelu", 
  "hidden_dropout_prob": 0.1, 
  "hidden_size": 768, 
  "initializer_range": 0.02, 
  "intermediate_size": 3072, 
  "max_position_embeddings": 512, 
  "num_attention_heads": 12, 
  "num_hidden_layers": 12, 
  "pooler_fc_size": 768, 
  "pooler_num_attention_heads": 12, 
  "pooler_num_fc_layers": 3, 
  "pooler_size_per_head": 128, 
  "pooler_type": "first_token_transform", 
  "type_vocab_size": 2, 
  "vocab_size": VOC_SIZE
}

with open("{}/bert_config.json".format(MODEL_DIR), "w") as fo:
  json.dump(bert_base_config, fo, indent=2)
  
with open("{}/{}".format(MODEL_DIR, VOC_FNAME), "w") as fo:
  for token in bert_vocab:
    fo.write(token+"\n")

if BUCKET_NAME:
  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME

!python bert/run_pretraining.py \
    --input_file gs://another_qa_bucket/pretraining_data/*.tfrecord \
    --output_dir gs://another_qa_bucket/bert_model_wiki/model/ \
    --do_train True \
    --do_eval True \
    --bert_config_file gs://another_qa_bucket/bert_model_wiki/bert_config.json \
    --train_batch_size 32 \
    --max_seq_length 128 \
    --masked_lm_prob 0.15\
    --max_predictions_per_seq 20 \
    --num_train_steps 1000000 \
    --num_warmup_steps 10 \
    --learning_rate 2e-5 \
    --use_tpu True \
    --tpu_name $TPU_NAME

json_data = {'data': [{'paragraphs': [{'context': 'HIS "Hightech Information System Limited" established 1987 , is a Hong Kong based graphics card manufacturer that produces AMD formerly known as ATI Radeon graphics cards.  Its headquarters are in Hong Kong, with additional sales offices and distribution networks in Europe, the Middle East, North America and Asia Pacific Regions. The current distributor in Hong Kong is JunMax Technology. Products HIS manufactures and sells AMD Radeon series video cards.  They are known for their IceQ cooling technology as well as producing the latest and fastest PCI cards like AMD Radeon RX 590, RX 5700 and RX 5700 XT.  In 2019, HIS launched new versions of the RX 5700 XT in pink and blue. External links HIS Ltd.',
     'qas': [{'answers': [{'answer_start': 0,
         'text': 'HIS "Hightech Information System Limited"'}],
       'question': 'What is the name of the company that produces AMD formerly known as ATI Radeon graphics cards?','id':1},
      {'answers': [{'answer_start': 54, 'text': '1987'}],
       'question': 'When was HIS "Hightech Information System Limited" established?','id':2},
      {'answers': [{'answer_start': 24, 'text': 'Hong Kong'}],
       'question': "Where is HIS' headquarters located?",'id':4},
      {'answers': [{'answer_start': 40, 'text': 'JunMax Technology'}],
       'question': 'What is the current distributor of AMD Radeon graphics cards in Hong Kong?','id':5},
      {'answers': [{'answer_start': 36, 'text': 'AMD Radeon'}],
       'question': 'What series of video cards does HIS manufacture and sell?','id':6},
      {'answers': [{'answer_start': 25, 'text': 'IceQ'}],
       'question': 'What type of cooling technology is HIS known for?','id':7},
      {'answers': [{'answer_start': 56, 'text': 'pink and blue'}],
       'question': 'In what colors did HIS launch the RX 5700 XT?','id':8},
      {'answers': [{'answer_start': 15, 'text': 'HIS Ltd.'}],
       'question': 'What is the name of the company that manufactures and sells AMD Radeon graphics cards?','id':9}]}],
   'title': 'good'},
  {'paragraphs': [{'context': 'HIS "Hightech Information System Limited" established 1987 , is a Hong Kong based graphics card manufacturer that produces AMD formerly known as ATI Radeon graphics cards.  Its headquarters are in Hong Kong, with additional sales offices and distribution networks in Europe, the Middle East, North America and Asia Pacific Regions. The current distributor in Hong Kong is JunMax Technology. Products HIS manufactures and sells AMD Radeon series video cards.  They are known for their IceQ cooling technology as well as producing the latest and fastest PCI cards like AMD Radeon RX 590, RX 5700 and RX 5700 XT.  In 2019, HIS launched new versions of the RX 5700 XT in pink and blue. External links HIS Ltd.',
     'qas': [{'answers': [{'answer_start': 0,
         'text': 'HIS "Hightech Information System Limited"'}],
       'question': 'What is the name of the company that produces AMD formerly known as ATI Radeon graphics cards?','id':10},
      {'answers': [{'answer_start': 54, 'text': '1987'}],
       'question': 'When was HIS "Hightech Information System Limited" established?','id':11},
      {'answers': [{'answer_start': 24, 'text': 'Hong Kong'}],
       'question': "Where is HIS' headquarters located?",'id':12},
      {'answers': [{'answer_start': 40, 'text': 'JunMax Technology'}],
       'question': 'What is the current distributor of AMD Radeon graphics cards in Hong Kong?','id':13},
      {'answers': [{'answer_start': 36, 'text': 'AMD Radeon'}],
       'question': 'What series of video cards does HIS manufacture and sell?','id':14},
      {'answers': [{'answer_start': 25, 'text': 'IceQ'}],
       'question': 'What type of cooling technology is HIS known for?','id':15},
      {'answers': [{'answer_start': 56, 'text': 'pink and blue'}],
       'question': 'In what colors did HIS launch the RX 5700 XT?','id':16},
      {'answers': [{'answer_start': 15, 'text': 'HIS Ltd.'}],
       'question': 'What is the name of the company that manufactures and sells AMD Radeon graphics cards?','id':17}]}],
   'title': 'second'}],
 'version': 'v1.1'}

with open('data.json', 'w') as outfile:
    json.dump(json_data, outfile)

!python bert/run_squad.py \
  --vocab_file gs://another_qa_bucket/bert_model_wiki/vocab.txt \
  --bert_config_file gs://another_qa_bucket/bert_model_wiki/bert_config.json \
  --init_checkpoint gs://another_qa_bucket/bert_model_wiki/model/model.ckpt-1000000 \
  --do_train=True \
  --train_file /content/train-v1.1.json \
  --do_predict=True \
  --predict_file /content/dev-v1.1.json \
  --train_batch_size 12 \
  --learning_rate 3e-5 \
  --num_train_epochs 2.0 \
  --max_seq_length 128 \
  --doc_stride 128 \
  --output_dir gs://another_qa_bucket/bert_model_wiki/

!python /content/bi-att-flow/squad/evaluate-v1.1.py /content/dev-v1.1.json /content/bert_model_wiki_predictions.json

!git clone https://github.com/allenai/bi-att-flow

!pip install pytorch_transformers

pip install transformers

from transformers import BertTokenizer

from transformers import BertForQuestionAnswering,BertConfig

configuration = BertConfig.from_json_file('/content/bert_model_wiki_bert_config.json')

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

model_s = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

#tokenizer = BertTokenizer.from_pretrained('/content/pytorch_model_pt.bin',config = configuration)
import codecs

codecs.register_error('strict', codecs.lookup_error('surrogateescape'))
model_trained = BertForQuestionAnswering.from_pretrained('/content/model.ckpt-14599.index')

!pip3 install transformers

!transformers-cli convert --model_type bert \
  --tf_checkpoint /content/model.ckpt-1000000 \
  --config /content/bert_model_wiki_bert_config.json \
  --pytorch_dump_output pytorch_model_pt.bin

!pip uninstall -y transformers

!pip install transformers

from pytorch_pretrained_bert import BertConfig, BertForTokenClassification, load_tf_weights_in_bert

# Initialise a configuration according to your model
config = BertConfig.from_json_file('/content/bert_config.json')


# Load weights from tf checkpoint
load_tf_weights_in_bert(model, "/content/model.ckpt-1000000.index")

pytorch_dump_path = "pytorch_model_pretrained.bin"

# Save pytorch-model
print("Save PyTorch model to {}".format(pytorch_dump_path))
torch.save(model.state_dict(), pytorch_dump_path)

!transformers-cli convert --model_type bert \
  --tf_checkpoint /content/model.ckpt-14599.index \
  --config /content/bert_model_wiki_bert_config.json \
  --pytorch_dump_output pytorch_model_new.bin

!pip install pytorch_pretrained_bert

!pip install torch

import torch
from pytorch_pretrained_bert import BertConfig, BertForTokenClassification, load_tf_weights_in_bert

# Initialise a configuration according to your model
config = BertConfig.from_json_file('/content/bert_model_wiki_bert_config.json')

# You will need to load a BertForTokenClassification model
model = BertForTokenClassification(config,2)

# Load weights from tf checkpoint
load_tf_weights_in_bert(model, "/content/model.ckpt-14599")

pytorch_dump_path = "/content/pytorch_model_finetuned"

# Save pytorch-model
print("Save PyTorch model to {}".format(pytorch_dump_path))
torch.save(model.state_dict(), pytorch_dump_path)

import os
import torch
import time
from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

from transformers import (
    AlbertConfig,
    AlbertForQuestionAnswering,
    AlbertTokenizer,
    squad_convert_examples_to_features
)

from transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample

from transformers.data.metrics.squad_metrics import compute_predictions_logits

# # READER NOTE: Set this flag to use own model, or use pretrained model in the Hugging Face repository
# use_own_model = False

# if use_own_model:
#   model_name_or_path = "/content/model_output"
# else:
#   model_name_or_path = "ktrapeznikov/albert-xlarge-v2-squad-v2"

# output_dir = ""

# # Config
# n_best_size = 1
# max_answer_length = 30
# do_lower_case = True
# null_score_diff_threshold = 0.0

# def to_list(tensor):
#     return tensor.detach().cpu().tolist()

# # Setup model
# config_class, model_class, tokenizer_class = (
#     AlbertConfig, AlbertForQuestionAnswering, AlbertTokenizer)
# config = config_class.from_pretrained(model_name_or_path)
# tokenizer = tokenizer_class.from_pretrained(
#     model_name_or_path, do_lower_case=True)
# model = model_class.from_pretrained(model_name_or_path, config=config)

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model.to(device)

# processor = SquadV2Processor()

def run_prediction(question_texts, context_text):
    """Setup function to compute predictions"""
    examples = []

    for i, question_text in enumerate(question_texts):
        example = SquadExample(
            qas_id=str(i),
            question_text=question_text,
            context_text=context_text,
            answer_text=None,
            start_position_character=None,
            title="Predict",
            is_impossible=False,
            answers=None,
        )

        examples.append(example)

    features, dataset = squad_convert_examples_to_features(
        examples=examples,
        tokenizer=tokenizer,
        max_seq_length=384,
        doc_stride=128,
        max_query_length=64,
        is_training=False,
        return_dataset="pt",
        threads=1,
    )

    eval_sampler = SequentialSampler(dataset)
    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)

    all_results = []

    for batch in eval_dataloader:
        model.eval()
        batch = tuple(t.to(device) for t in batch)

        with torch.no_grad():
            inputs = {
                "input_ids": batch[0],
                "attention_mask": batch[1],
                "token_type_ids": batch[2],
            }

            example_indices = batch[3]

            outputs = model(**inputs)

            for i, example_index in enumerate(example_indices):
                eval_feature = features[example_index.item()]
                unique_id = int(eval_feature.unique_id)

                output = [to_list(output[i]) for output in outputs]

                start_logits, end_logits = output
                result = SquadResult(unique_id, start_logits, end_logits)
                all_results.append(result)

    output_prediction_file = "predictions.json"
    output_nbest_file = "nbest_predictions.json"
    output_null_log_odds_file = "null_predictions.json"

    predictions = compute_predictions_logits(
        examples,
        features,
        all_results,
        n_best_size,
        max_answer_length,
        do_lower_case,
        output_prediction_file,
        output_nbest_file,
        output_null_log_odds_file,
        False,  # verbose_logging
        True,  # version_2_with_negative
        null_score_diff_threshold,
        tokenizer,
    )

    return predictions

context = "New Zealand (Māori: Aotearoa) is a sovereign island country in the southwestern Pacific Ocean. It has a total land area of 268,000 square kilometres (103,500 sq mi), and a population of 4.9 million. New Zealand's capital city is Wellington, and its most populous city is Auckland."
questions = ["How many people live in New Zealand?", 
             "What's the largest city?"]

# Run method
predictions = run_prediction(questions, context)

# Print results
for key in predictions.keys():
  print(predictions[key])

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer.encode(question, text)
start_positions = torch.tensor([1])
print(start_positions)
end_positions = torch.tensor([3])
print(end_positions)
outputs = model(**inputs,start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits

input_text = "[CLS] " + question + " [SEP] " + text + " [SEP]"
input_ids = tokenizer.encode(input_text)
print(len(input_ids))
token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]
start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))
all_tokens = tokenizer.convert_ids_to_tokens(input_ids)
print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))
# a nice puppet
tokenizer.decode(input_ids)
#'[CLS] [CLS] who was jim henson? [SEP] jim henson was a nice puppet [SEP] [SEP]'

questions = 'When was the HIS "Hightech Information System Limited" established ?'
texts = 'HIS "Hightech Information System Limited" established 1987 , is a Hong Kong based graphics card manufacturer that produces AMD formerly known as ATI Radeon graphics cards.  Its headquarters are in Hong Kong, with additional sales offices and distribution networks in Europe, the Middle East, North America and Asia Pacific Regions. The current distributor in Hong Kong is JunMax Technology. Products HIS manufactures and sells AMD Radeon series video cards.  They are known for their IceQ cooling technology as well as producing the latest and fastest PCI cards like AMD Radeon RX 590, RX 5700 and RX 5700 XT.  In 2019, HIS launched new versions of the RX 5700 XT in pink and blue. External links HIS Ltd.'
#input_text = "[CLS] " + question + " [SEP] " + text + " [SEP]"
#print(input_text)
input_id = tokenizer.encode(questions,texts)
print(input_id)
print('The input has a total of {:} tokens.'.format(len(input_id)))

tokens = tokenizer.convert_ids_to_tokens(input_id)
print(tokens)
# For each token and its id...
for token, id in zip(tokens, input_id):
    
    # If this is the [SEP] token, add some space around it to make it stand out.
    if id == tokenizer.sep_token_id:
        print('')
    
    # Print the token string and its ID in two columns.
    print('{:<12} {:>6,}'.format(token, id))

    if id == tokenizer.sep_token_id:
        print('')

sep_index = input_id.index(tokenizer.sep_token_id)

# The number of segment A tokens includes the [SEP] token istelf.
num_seg_a = sep_index + 1
# The remainder are segment B.
num_seg_b = len(input_id) - num_seg_a

# Construct the list of 0s and 1s.
segment_ids = [0]*num_seg_a + [1]*num_seg_b
print(segment_ids)

# There should be a segment_id for every input token.
assert len(segment_ids) == len(input_id)

pip install torch

import torch
start_score, end_score = model_s(torch.tensor([input_id]),token_type_ids=torch.tensor([segment_ids]),return_dict=False) # The segment IDs to differentiate question from answer_text

# Find the tokens with the highest `start` and `end` scores.
print(start_score)
print(end_score)
answer_starts = torch.argmax(start_score)
print(answer_starts)
answer_end = torch.argmax(end_score)
print(answer_end)

# Combine the tokens in the answer and print it out.
answer = ' '.join(tokens[answer_starts:answer_end])

print('Answer: "' + answer + '"')

tokens[answer_start:answer_end+1]

input_ids = tokenizer.encode(input_text, add_special_tokens=False)
print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))
# was a

all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]

